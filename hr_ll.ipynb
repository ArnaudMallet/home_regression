{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Module list"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nimport time\nimport datetime\n\n!pip install torch-summary\nfrom torchsummary import summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conda check"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if CUDA is available\nuse_cuda = torch.cuda.is_available()\nprint(use_cuda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inputs & Variables\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the set-up for pandas dataframe\npd.set_option('display.max_rows', None)\n\n# Train CSV dataset - length 1460\ntrain_csv = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n\n# Test CSV dataset - length 1459\ntest_csv = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\n# submission CSV file\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n\n# Directory \ndata_dir = \"../input/house-prices-advanced-regression-techniques/\"\n\n# Date and Time\ndate_res = datetime.datetime.now()\n\n# Define Version\n#VERSION = 'version_'+date_res.strftime(\"%m/%d/%Y\")+'.pt'\nVERSION = 'CNN_v1.pt'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train set description"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"dtf_description = train_csv.describe()\ndtf_description","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv.info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean for the non-dummy entries\ndtf_description.loc['mean']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Number of entries that are null/void"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_csv.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function to split training and validation data from the training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_valid_data(data, perc=0.75):\n    return data.head(int(len(data)*(perc)))\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creation of the Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, csv_file_data, csv_file_test, id_col, target_col, data=None):\n        self.data_train= pd.read_csv(csv_file_data)\n        self.data_test = pd.read_csv(csv_file_test)\n        self.id        = id_col\n        self.target    = target_col\n        self.data = data\n\n    def __len__(self):\n        if self.data == 'train':\n            return len(self.data_train)\n        else:\n            return len(self.data_test)\n\n    def __getitem__(self, idx):\n        # remove the target column\n        train_wo_SP = self.data_train.drop(self.target, axis='columns')\n        # concat train and test features to have the same number of columns one the dummies features appear\n        all_features = pd.concat([train_wo_SP, self.data_test], keys=[\"train\", \"test\"])\n        # Normalize the numerical features\n        numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n        all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n        \n        # creathe the dummies for train and test set\n        all_features_dummies = pd.get_dummies(all_features)\n        all_features_dummies = all_features_dummies.fillna(0)\n        #print(all_features_dummies.shape)\n        # all features_dummies is 1460 x 289 for the train set and 1459 x 289 for the test set\n        \n        # creation of the label of train dataset\n        train_label1 = self.data_train['Id']\n        train_label2 = self.data_train['SalePrice']\n        train_label = pd.DataFrame(columns = ['Id', 'SalePrice'])\n        train_label['Id'] = train_label1\n        train_label['SalePrice'] = train_label2\n\n        #Split Data - creation of the Validation dataset\n        train_data = split_train_valid_data(all_features_dummies.loc['train'])\n        valid_data = all_features_dummies.loc['train'].iloc[max(train_data.index+1):]\n\n        \n        #Split label - creation of the validation labelset\n        label_train = split_train_valid_data(train_label)\n        label_valid = train_label.iloc[max(train_data.index+1):]\n         \n        # creation of the test data set\n        test_data = all_features_dummies.loc['test']\n        \n        # creation of an Empty label test\n        label_test = pd.DataFrame(np.empty((test_data.shape[0],1)))\n        \n        train_data = train_data.astype(np.float32)\n        valid_data = valid_data.astype(np.float32)\n        test_data = test_data.astype(np.float32)\n        label_train = label_train.astype(np.float32)\n        label_valid = label_valid.astype(np.float32)\n        \n        # remove 'ID' columns - data\n        train_data = train_data.drop(['Id'], axis=1)\n        #print(len(train_data))\n        valid_data = valid_data.drop(['Id'],axis=1)\n        #print(len(valid_data))\n        test_data = test_data.drop(['Id'], axis=1)\n        #print(len(test_data))\n        \n        # remove 'ID' column - label\n        label_train = label_train.drop(['Id'], axis=1)\n        label_valid = label_valid.drop(['Id'], axis=1)\n            \n        # data preparation\n        if self.data == 'train':\n            use_data = train_data.to_numpy()\n            use_data = torch.from_numpy(use_data)\n        elif self.data == 'valid':\n            use_data = valid_data.to_numpy()\n            use_data = torch.from_numpy(use_data)\n        elif self.data == 'test':\n            use_data = test_data.to_numpy()\n            use_data = torch.from_numpy(use_data)\n            \n        # label preparation\n        if self.data == 'train':\n            label_data = label_train.to_numpy()\n            label_data = torch.from_numpy(label_data)\n        elif self.data == 'valid':\n            label_data = label_valid.to_numpy()\n            label_data = torch.from_numpy(label_data)\n        elif self.data == 'test':\n            label_data = label_test.to_numpy()\n            label_data = torch.from_numpy(label_data)\n        \n        return use_data, label_data\n\nparams = {\n    'id_col':'Id',  \n    'target_col': ['SalePrice'],\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dataset = {x: CustomDataset(csv_file_data=data_dir+'train.csv', \n                                 csv_file_test=data_dir+'test.csv', \n                                 **params, \n                                 data='train' if x == 'train' \n                                 else 'valid' if x =='valid' \n                                 else 'test') \n                for x in ['train', 'valid', 'test']\n               }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader = {x :torch.utils.data.DataLoader(data_dataset[x], batch_size=1, shuffle=True)\n                for x in ['train', 'valid', 'test']}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('TRAINING')\n\ndata, lab_target = data_dataset['train'].__getitem__(0)\n\nprint('DATASET')\nprint('Data shape: ', data.shape)\nprint('Data type: ', type(data))\nprint('Data size: {}'.format(data.size()))\n#print('Exampe of the feature for the 1st entry {}'.format(data[0]))\nprint('\\nTarget at the first row: {}'.format(lab_target.size()))\nprint('Example of the label for the 1st entry: {}'.format(lab_target[0]))\n\n\nprint()\nprint('Train Loader type')\ntrain_iter = iter(data_loader['train'])\nprint(type(train_iter))\n\ndatas, labels_target = train_iter.next()\n\nprint('DATALOADER')\nprint('images shape on batch size = ', datas.size())\nprint('Example of datas for the 1st entry {}'.format(datas[0].size()))\n#print('\\nTaregt type on batch size = {}'.format(labels_target))\nprint('Target type on batch size = {}'.format(type(labels_target)))\nprint('Target shape on batch size = ', labels_target.shape)\nprint(len(train_iter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''for idx, (data, target) in enumerate(data_loader['train']):\n    print(data[0][idx])'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    ### TODO: choose an architecture, and complete the class\n    def __init__(self):\n        super(Net, self).__init__()\n        ## Define linear layers\n        \n        # linear layer (330 -> 755001)\n        self.fc1 = nn.Linear(288, 100)\n        # linear layer (500 -> 250)\n        self.fc2 = nn.Linear(100, 50)\n        # linear layer (250 -> 125)\n        self.fc3 = nn.Linear(50, 1)\n\n        self.dropout = nn.Dropout(0.1)\n        \n        # LogSoftmax\n        self.RELU = nn.ReLU()\n    \n    def forward(self, x):\n        \n        #print('before l1 {}'.format(x))\n        # 1LL\n        x = torch.tanh(self.fc1(x))\n        x = self.dropout(x)\n        \n        #print('before l2 {}'.format(x))\n        # 2LL\n        x = torch.tanh(self.fc2(x))\n        x = self.dropout(x)\n        \n        #print('before l3 {}'.format(x))\n        # 3LL\n        x = self.RELU(self.fc3(x))\n        \n        #print('After LR {}'.format(x))\n        return x\n\n#-#-# You do NOT have to modify the code below this line. #-#-#\n\n# instantiate the CNN\nmodel_HR = Net()\n\n# move tensors to GPU if CUDA is available\nif use_cuda:\n    model_HR.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_HR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(model_HR.parameters()).is_cuda","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Torch Summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(Net(),input_size=(1, 288))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Learning rate\nlr=0.001\n\n#momentum\nmomentum = 0.8\n\n### select loss function\ncriterion = nn.MSELoss()\n\n# other Loss function\n# 1-MSE\n\n# 2-Mae\ndef mae(true, pred):\n    return np.sum(np.abs(true - pred))\n\n# 3-huber loss\ndef huber(true, pred, delta):\n    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))\n    return np.sum(loss)\n\n# 4-log cosh loss\ndef logcosh(true, pred):\n    loss = np.log(np.cosh(pred - true))\n    return np.sum(loss)\n\n# 5-Quantile Loss / Note: the Quantile Loss is not define yet\n\n### select optimizer\n\n\noptimizer = optim.SGD(model_HR.parameters(), lr=lr, momentum = momentum)\n\n#other otpions: \n# SGD\n# optim.SGD(model_HR.parameters(), lr=lr, momentum = momemtum)\n# Sparse Adam - In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.\n# optim.SparseAdam(model_HR.parameters(), lr=lr)\n# Average Stochastic gradient descent\n# optim.ASGD(model_HR.parameters(), lr=lr)\n# RMSprop\n# optim.RMSprop(model_HR.parameters(), lr=lr, momentum = momemtum)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfor data in iter(data_loader['train']):\n    print(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(n_epochs, loaders, model, optimizer, criterion):\n    \"\"\"returns trained model\"\"\"\n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf \n    time_start = time.time()\n    train_class = []\n    valid_class = []\n    epoch_class = []\n    \n    for epoch in range(1, n_epochs+1):\n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        \n        ###################\n        # train the model #\n        ###################\n        model.train()\n        for idx, (data, target) in enumerate(loaders['train']):\n            data = data[0][idx]\n            target = target[0][idx]\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            output = model(data)\n            '''for name, param in model.named_parameters(): \n                if param.requires_grad: \n                    print(name, param.data)'''\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            train_loss = criterion(output, target)           \n\n        model.eval()\n        for idx, (data, target) in enumerate(loaders['valid']):\n            data = data[0][idx]\n            target = target[0][idx]\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n            ## update the average validation loss\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # update average validation loss \n            valid_loss = criterion(output, target)\n        \n        # Next todo, create a variable lr according to validation loss\n        '''if valid_loss < 0.35 and valid_loss > 0.15:\n            LR=0.005\n        elif valid_loss < 0.15:\n            LR=0.001'''\n        \n        # Calcul time\n        time_now = time.time()\n        \n        time_epoch = (time_now - time_start)/60\n            \n        # print training/validation statistics \n        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTime since the beginning {:.1f} min \\tLearning rate: {:.6f} '.format(\n            epoch, \n            train_loss,\n            valid_loss,\n            time_epoch,\n            lr\n            ))\n        \n        ## save the model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss,\n            torch.save(model.state_dict(), VERSION))\n                 )\n            valid_loss_min = valid_loss\n        \n        # store class data\n        train_class.append(train_loss)\n        valid_class.append(valid_loss)\n        epoch_class.append(epoch)\n    \n    plt.plot(epoch_class, train_class, 'g', label='Training loss')\n    plt.plot(epoch_class, valid_class, 'b', label='validation loss')\n    plt.title('Training and Validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \n    \n    # return trained model\n    return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"model_res = train(5, data_loader, model_HR, optimizer, criterion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result and submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(len(data_loader['test']))\ndataiter_test = iter(data_loader['test'])\nprint('data_iter\\n',dataiter_test)\ndatas, labels = dataiter_test.next()\nprint('images\\n',datas.shape)\nprint('labels\\n',labels.shape)\n\nmodel_test = model_HR\nmodel_test.load_state_dict(torch.load('/kaggle/working/'+VERSION))\nmodel_test = model_test.eval()\nprint(model_test)\n\nif use_cuda:\n    datas = datas.cuda()\nout_fwd = model_test.forward(datas)\nprint(out_fwd)\n#print('Result preditcion model on dataset:\\n {}\\n'.format(out_fwd))\n#probs = torch.exp(out_fwd)\n#print('probs\\n', probs)\n#print(probs.max())\n#print(probs.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(out_fwd)):\n    submission.iloc[i][1] = out_fwd[i][0]\n\npd.set_option(\"display.max_rows\", 10, \"display.max_columns\", None)\n\nsubmission.to_csv(path_or_buf='sample_submission_'+VERSION+'.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Score\n\nresult: - Ranking: "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}