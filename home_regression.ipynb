{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "train_csv = pd.read_csv(\"train.csv\")\n",
    "test_csv = pd.read_csv(\"test.csv\")\n",
    "samples_submission_csv = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "        Id  SalePrice\n",
      "0        1     208500\n",
      "1        2     181500\n",
      "2        3     223500\n",
      "3        4     140000\n",
      "4        5     250000\n",
      "...    ...        ...\n",
      "1455  1456     175000\n",
      "1456  1457     210000\n",
      "1457  1458     266500\n",
      "1458  1459     142125\n",
      "1459  1460     147500\n",
      "\n",
      "[1460 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# creation label based on train set\n",
    "#print(train_csv.dtypes)\n",
    "train_label1 = train_csv['Id']\n",
    "print(type(train_label1))\n",
    "train_label2 = train_csv['SalePrice']\n",
    "print(type(train_label2))\n",
    "train_label = pd.DataFrame(columns = ['Id', 'SalePrice'])\n",
    "train_label['Id'] = train_label1\n",
    "train_label['SalePrice'] = train_label2\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1460, 80)\n"
     ]
    }
   ],
   "source": [
    "# Trainin set\n",
    "print(train_csv.shape)\n",
    "train_wo_SP = train_csv.drop('SalePrice', axis='columns')\n",
    "print(train_wo_SP.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.concat([train_wo_SP, test_csv], keys=[\"train\", \"test\"])\n",
    "\n",
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "all_features_dummies = pd.get_dummies(all_features)\n",
    "\n",
    "#print(all_features_dummies.shape)\n",
    "print(type(all_features_dummies.loc['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "def split_train_valid_data(data, perc=0.7):\n",
    "    import pandas as pd\n",
    "    return data.head(int(len(data)*(perc)))\n",
    "\n",
    "train_data = split_train_valid_data(all_features_dummies.loc['train'])\n",
    "valid_data = all_features_dummies.loc['train'].iloc[max(train_data.index+1):]\n",
    "\n",
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  SalePrice\n",
      "1021  1022     194000\n",
      "1022  1023      87000\n",
      "1023  1024     191000\n",
      "1024  1025     287000\n",
      "1025  1026     112500\n",
      "...    ...        ...\n",
      "1455  1456     175000\n",
      "1456  1457     210000\n",
      "1457  1458     266500\n",
      "1458  1459     142125\n",
      "1459  1460     147500\n",
      "\n",
      "[439 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# same for label\n",
    "\n",
    "label_train = split_train_valid_data(train_label)\n",
    "label_valid = train_label.iloc[max(train_data.index+1):]\n",
    "\n",
    "print(label_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "0      0.000000e+00\n",
      "1     4.940656e-324\n",
      "2     9.881313e-324\n",
      "3     1.482197e-323\n",
      "4     1.976263e-323\n",
      "...             ...\n",
      "1454  7.183714e-321\n",
      "1455  7.188655e-321\n",
      "1456  7.193596e-321\n",
      "1457  7.198536e-321\n",
      "1458  7.203477e-321\n",
      "\n",
      "[1459 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "test_data = all_features_dummies.loc['test']\n",
    "#print(test_data.shape[0])\n",
    "\n",
    "label_test = pd.DataFrame(np.empty((test_data.shape[0],1)))\n",
    "print(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "(1021, 289)\n",
      "(439, 289)\n",
      "(1021, 2)\n",
      "(439, 2)\n",
      "(1459, 289)\n",
      "(1459, 1)\n",
      "after\n",
      "(1021, 288)\n",
      "(439, 288)\n",
      "(1021, 1)\n",
      "(439, 1)\n",
      "(1459, 288)\n",
      "(1459, 1)\n"
     ]
    }
   ],
   "source": [
    "# Recap\n",
    "print('before')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(label_train.shape)\n",
    "print(label_valid.shape)\n",
    "print(test_data.shape)\n",
    "print(label_test.shape)\n",
    "\n",
    "print('after')\n",
    "train_data = train_data.drop(['Id'], axis=1)\n",
    "print(train_data.shape)\n",
    "valid_data = valid_data.drop(['Id'],axis=1)\n",
    "print(valid_data.shape)\n",
    "label_train = label_train.drop(['Id'], axis=1)\n",
    "print(label_train.shape)\n",
    "label_valid = label_valid.drop(['Id'], axis=1)\n",
    "print(label_valid.shape)\n",
    "test_data = test_data.drop(['Id'], axis=1)\n",
    "print(test_data.shape)\n",
    "print(label_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SalePrice\n",
      "0        208500\n",
      "1        181500\n",
      "2        223500\n",
      "3        140000\n",
      "4        250000\n",
      "...         ...\n",
      "1016     203000\n",
      "1017     187500\n",
      "1018     160000\n",
      "1019     213490\n",
      "1020     176000\n",
      "\n",
      "[1021 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       208500.0\n",
      "1       181500.0\n",
      "2       223500.0\n",
      "3       140000.0\n",
      "4       250000.0\n",
      "          ...   \n",
      "1016    203000.0\n",
      "1017    187500.0\n",
      "1018    160000.0\n",
      "1019    213490.0\n",
      "1020    176000.0\n",
      "Name: SalePrice, Length: 1021, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amallet\\Anaconda\\envs\\udacity_env\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_train_label = pd.concat([train_data, label_train], keys=[\"data\", \"label\"])\n",
    "print(data_train_label.loc['label']['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file_data, csv_file_test, id_col, target_col, data='train'):\n",
    "        self.data_train= pd.read_csv(csv_file_data)\n",
    "        seff.data_test = pd.read_csv(csv_file_test)\n",
    "        self.id        = id_col\n",
    "        self.target    = target_col\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # remove the target column\n",
    "        train_wo_SP = self.data.drop(self.target, axis='columns')\n",
    "        # concat train and test features to have the same number of columns one the dummies features appear\n",
    "        all_features = pd.concat([train_wo_SP, test_csv], keys=[\"train\", \"test\"])\n",
    "        # Normalize the numerical features\n",
    "        numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "        all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "        # creathe the dummies for train and test set\n",
    "        all_features_dummies = pd.get_dummies(all_features)\n",
    "        \n",
    "        # creation of the label of train dataset\n",
    "        train_label1 = train_csv['Id']\n",
    "        train_label2 = train_csv['SalePrice']\n",
    "        train_label = pd.DataFrame(columns = ['Id', 'SalePrice'])\n",
    "        train_label['Id'] = train_label1\n",
    "        train_label['SalePrice'] = train_label2\n",
    "\n",
    "        #Split Data - creation of the Validation dataset\n",
    "        train_data = split_train_valid_data(all_features_dummies.loc['train'])\n",
    "        valid_data = all_features_dummies.loc['train'].iloc[max(train_data.index+1):]\n",
    "        #Split label - creation of the validation labelset\n",
    "        label_train = split_train_valid_data(train_label)\n",
    "        label_valid = train_label.iloc[max(train_data.index+1):]\n",
    "         \n",
    "        # creation of the test data set\n",
    "        test_data = all_features_dummies.loc['test']\n",
    "        \n",
    "        # creation of an Empty label test\n",
    "        label_test = pd.DataFrame(np.empty((test_data.shape[0],1)))\n",
    "        \n",
    "        # remove 'ID' columns - data\n",
    "        train_data = train_data.drop(['Id'], axis=1)\n",
    "        valid_data = valid_data.drop(['Id'],axis=1)\n",
    "        test_data = test_data.drop(['Id'], axis=1)\n",
    "        \n",
    "        # remove 'ID' column - label\n",
    "        label_train = label_train.drop(['Id'], axis=1)\n",
    "        label_valid = label_valid.drop(['Id'], axis=1)\n",
    "            \n",
    "        # data preparation\n",
    "        if self.data == 'train':\n",
    "            use_data = train_data.to_numpy()\n",
    "            use_data = torch.from_numpy(use_data)\n",
    "        else self.data == 'valid':\n",
    "            use_data = valid_data.to_numpy()\n",
    "            use_data = torch.from_numpy(use_data)\n",
    "        else self.data == 'test':\n",
    "            use_data = test_data.to_numpy()\n",
    "            use_data = torch.from_numpy(use_data)\n",
    "            \n",
    "        # label preparation\n",
    "        if self.data == 'train':\n",
    "            label_data = label_train.to_numpy()\n",
    "            label_data = torch.from_numpy(label_data)\n",
    "        else self.data == 'valid':\n",
    "            label_data = label_valid.to_numpy()\n",
    "            label_data = torch.from_numpy(label_data)\n",
    "        else self.data == 'test':\n",
    "            label_data = label_test.to_numpy()\n",
    "            label_data = torch.from_numpy(label_data)\n",
    "        \n",
    "        return use_data, label_data\n",
    "\n",
    "params = {\n",
    "    'root_dir': ''\n",
    "    'id_col':     'Id',  \n",
    "    'target_col': ['SalePrice'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "class PrepareData(Dataset):\n",
    "\n",
    "    def __init__(self, In, Out):\n",
    "        if not torch.is_tensor(In):\n",
    "            In = In.to_numpy()\n",
    "            self.In = torch.from_numpy(In)\n",
    "        if not torch.is_tensor(Out):\n",
    "            Out = Out.to_numpy()\n",
    "            self.Out = torch.from_numpy(Out)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.In)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.In[idx], self.Out[idx]\n",
    "\n",
    "data_dataset = {x: PrepareData(In=train_data if x == 'train'\n",
    "                               else valid_data if x =='valid'\n",
    "                               else test_data, \n",
    "                               Out=label_train if x == 'train'\n",
    "                               else label_valid)\n",
    "                for x in ['train', 'valid', 'test']}\n",
    "\n",
    "data_loader = {x: torch.utils.data.DataLoader(data_dataset[x], batch_size = 10,  shuffle=False) \n",
    "               for x in ['train', 'valid', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x239d57e96c8>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dataset_2 = {x: CustomDataset(csv_file_data= , \n",
    "                                   csv_file_test, \n",
    "                                   id_col, \n",
    "                                   target_col, \n",
    "                                   data='train' if x == 'train'\n",
    "                                   else 'valid' if x =='valid'\n",
    "                                   else 'test')\n",
    "                for x in ['train', 'valid', 'test']}\n",
    "\n",
    "data_loader_2 = {x: torch.utils.data.DataLoader(data_dataset[x], batch_size = 10,  shuffle=False) \n",
    "               for x in ['train', 'valid', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        \n",
    "        # linear layer (330 -> 500)\n",
    "        self.fc1 = nn.Linear(330, 500)\n",
    "        \n",
    "        # linear layer (500 -> 250)\n",
    "        self.fc2 = nn.Linear(500, 250)\n",
    "        \n",
    "        # linear layer (250 -> 125)\n",
    "        self.fc3 = nn.Linear(250, 125)\n",
    "        \n",
    "        # linear layer (125 -> 1)\n",
    "        self.fc4 = nn.Linear(125, 1)\n",
    "        \n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.175)\n",
    "        \n",
    "        # LogSoftmax\n",
    "        self.LSM = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #h2\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #h3\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        #h4\n",
    "        x = self.fc4(x)\n",
    "        x = self.LSM(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_HR = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_patho.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=330, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc3): Linear(in_features=250, out_features=125, bias=True)\n",
       "  (fc4): Linear(in_features=125, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.175, inplace=False)\n",
       "  (LSM): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: select loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer = optim.SGD(model_HR.parameters(), lr=0.01, momentum = 0.9)\n",
    "\n",
    "VERSION = 'Test_version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    time_start = time.time()\n",
    "    train_class = []\n",
    "    valid_class = []\n",
    "    epoch_class = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        LR = 0.01\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for idx, (data, target,_) in enumerate(loaders['train']):\n",
    "\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            optim.SGD(model.parameters(), lr=LR, momentum=0.9).zero_grad()\n",
    "            # add data.float() as the data is in ByteTensor format, which doesn't work the model data type\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            #optimizer.step()\n",
    "            optim.SGD(model.parameters(), lr=LR, momentum=0.9).step()\n",
    "            #update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        model.eval()\n",
    "        for idx, (data, target, _) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(loaders['train'].sampler)\n",
    "        \n",
    "        \n",
    "        valid_loss = valid_loss/len(loaders['valid'].sampler)\n",
    "        \n",
    "        if valid_loss < 0.35 and valid_loss > 0.15:\n",
    "            LR=0.005\n",
    "        elif valid_loss < 0.15:\n",
    "            LR=0.001\n",
    "        \n",
    "        \n",
    "        # Calcul time\n",
    "        time_now = time.time()\n",
    "        \n",
    "        time_epoch = (time_now - time_start)/60\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTime since the beginning {:.1f} min \\tLearning rate: {:.6f} '.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            time_epoch,\n",
    "            LR\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss,\n",
    "            torch.save(model.state_dict(), VERSION))\n",
    "                 )\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "        # store class data\n",
    "        train_class.append(train_loss)\n",
    "        valid_class.append(valid_loss)\n",
    "        epoch_class.append(epoch)\n",
    "    \n",
    "    plt.plot(epoch_class, train_class, 'g', label='Training loss')\n",
    "    plt.plot(epoch_class, valid_class, 'b', label='validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # return trained model\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
