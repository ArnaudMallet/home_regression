{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "train_csv = pd.read_csv(\"train.csv\")\n",
    "test_csv = pd.read_csv(\"test.csv\")\n",
    "samples_submission_csv = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf_description = train_csv.describe()\n",
    "dtf_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.04995836802665"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use mean and column to replace the NA or field that are NAN instead to put thm to 0\n",
    "dtf_description.loc['mean']['LotFrontage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf_description.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            0\n",
       "LotFrontage       259\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1369\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           0\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "Exterior2nd         0\n",
       "MasVnrType          8\n",
       "MasVnrArea          8\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "BsmtQual           37\n",
       "BsmtCond           37\n",
       "BsmtExposure       38\n",
       "BsmtFinType1       37\n",
       "BsmtFinSF1          0\n",
       "BsmtFinType2       38\n",
       "BsmtFinSF2          0\n",
       "BsmtUnfSF           0\n",
       "TotalBsmtSF         0\n",
       "Heating             0\n",
       "HeatingQC           0\n",
       "CentralAir          0\n",
       "Electrical          1\n",
       "1stFlrSF            0\n",
       "2ndFlrSF            0\n",
       "LowQualFinSF        0\n",
       "GrLivArea           0\n",
       "BsmtFullBath        0\n",
       "BsmtHalfBath        0\n",
       "FullBath            0\n",
       "HalfBath            0\n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         0\n",
       "TotRmsAbvGrd        0\n",
       "Functional          0\n",
       "Fireplaces          0\n",
       "FireplaceQu       690\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageCars          0\n",
       "GarageArea          0\n",
       "GarageQual         81\n",
       "GarageCond         81\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1453\n",
       "Fence            1179\n",
       "MiscFeature      1406\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "SaleCondition       0\n",
       "SalePrice           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_data(data, perc=0.7):\n",
    "    return data.head(int(len(data)*(perc)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file_data, csv_file_test, id_col, target_col, data='train'):\n",
    "        self.data_train= pd.read_csv(csv_file_data)\n",
    "        self.data_test = pd.read_csv(csv_file_test)\n",
    "        self.id        = id_col\n",
    "        self.target    = target_col\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data == 'train':\n",
    "            return len(self.data_train)\n",
    "        else:\n",
    "            return len(self.data_test)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # remove the target column\n",
    "        train_wo_SP = self.data_train.drop(self.target, axis='columns')\n",
    "        # concat train and test features to have the same number of columns one the dummies features appear\n",
    "        all_features = pd.concat([train_wo_SP, self.data_test], keys=[\"train\", \"test\"])\n",
    "        # Normalize the numerical features\n",
    "        numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "        all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "        # creathe the dummies for train and test set\n",
    "        all_features_dummies = pd.get_dummies(all_features)\n",
    "        all_features_dummies = all_features_dummies.fillna(0)\n",
    "        # all features_dummies is 1460 x 289 for the train set and 1459 x 289 for the test set\n",
    "        \n",
    "        # creation of the label of train dataset\n",
    "        train_label1 = self.data_train['Id']\n",
    "        train_label2 = self.data_train['SalePrice']\n",
    "        train_label = pd.DataFrame(columns = ['Id', 'SalePrice'])\n",
    "        train_label['Id'] = train_label1\n",
    "        train_label['SalePrice'] = train_label2\n",
    "\n",
    "        #Split Data - creation of the Validation dataset\n",
    "        train_data = split_train_valid_data(all_features_dummies.loc['train'])\n",
    "        valid_data = all_features_dummies.loc['train'].iloc[max(train_data.index+1):]\n",
    "        \n",
    "        #Split label - creation of the validation labelset\n",
    "        label_train = split_train_valid_data(train_label)\n",
    "        label_valid = train_label.iloc[max(train_data.index+1):]\n",
    "         \n",
    "        # creation of the test data set\n",
    "        test_data = all_features_dummies.loc['test']\n",
    "        \n",
    "        # creation of an Empty label test\n",
    "        label_test = pd.DataFrame(np.empty((test_data.shape[0],1)))\n",
    "        \n",
    "        train_data = train_data.astype(np.float32)\n",
    "        valid_data = valid_data.astype(np.float32)\n",
    "        test_data = test_data.astype(np.float32)\n",
    "        label_train = label_train.astype(np.float32)\n",
    "        label_valid = label_valid.astype(np.float32)\n",
    "        \n",
    "        # remove 'ID' columns - data\n",
    "        train_data = train_data.drop(['Id'], axis=1)\n",
    "        valid_data = valid_data.drop(['Id'],axis=1)\n",
    "        test_data = test_data.drop(['Id'], axis=1)\n",
    "        \n",
    "        # remove 'ID' column - label\n",
    "        label_train = label_train.drop(['Id'], axis=1)\n",
    "        label_valid = label_valid.drop(['Id'], axis=1)\n",
    "            \n",
    "        # data preparation\n",
    "        if self.data == 'train':\n",
    "            use_data = train_data.to_numpy()\n",
    "            use_data = torch.from_numpy(use_data)\n",
    "        elif self.data == 'valid':\n",
    "            use_data = valid_data.to_numpy()\n",
    "            use_data = torch.from_numpy(use_data)\n",
    "        elif self.data == 'test':\n",
    "            use_data = test_data.to_numpy()\n",
    "            use_data = torch.from_numpy(use_data)\n",
    "            \n",
    "        # label preparation\n",
    "        if self.data == 'train':\n",
    "            label_data = label_train.to_numpy()\n",
    "            label_data = torch.from_numpy(label_data)\n",
    "        elif self.data == 'valid':\n",
    "            label_data = label_valid.to_numpy()\n",
    "            label_data = torch.from_numpy(label_data)\n",
    "        elif self.data == 'test':\n",
    "            label_data = label_test.to_numpy()\n",
    "            label_data = torch.from_numpy(label_data)\n",
    "        \n",
    "        return use_data, label_data\n",
    "\n",
    "params = {\n",
    "    'id_col':'Id',  \n",
    "    'target_col': ['SalePrice'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dataset = {x: CustomDataset(csv_file_data=\"train.csv\" , \n",
    "                                   csv_file_test=\"test.csv\", \n",
    "                                   **params, \n",
    "                                   data='train' if x == 'train'\n",
    "                                   else 'valid' if x =='valid'\n",
    "                                   else 'test')\n",
    "                for x in ['train', 'valid', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = {x :torch.utils.data.TensorDataset(data_dataset[x][0][0], data_dataset[x][0][1])\n",
    "                for x in ['train', 'valid', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021\n"
     ]
    }
   ],
   "source": [
    "print(len(data_loader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "DATASET\n",
      "image at the first row:  torch.Size([1021, 288])\n",
      "image at the first row:  <class 'torch.Tensor'>\n",
      "image size at the first row: torch.Size([1021, 288])\n",
      "\n",
      "Target at the first row:  tensor([[208500.],\n",
      "        [181500.],\n",
      "        [223500.],\n",
      "        ...,\n",
      "        [160000.],\n",
      "        [213490.],\n",
      "        [176000.]])\n",
      "Target format at the first row: tensor([[208500.],\n",
      "        [181500.],\n",
      "        [223500.],\n",
      "        ...,\n",
      "        [160000.],\n",
      "        [213490.],\n",
      "        [176000.]])\n",
      "Target format at the first row: torch.Size([1021, 1])\n",
      "\n",
      "Train Loader type\n",
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "DATALOADER\n",
      "images type on batch size = <class 'torch.Tensor'>\n",
      "images shape on batch size =  torch.Size([288])\n",
      "\n",
      "Targett type on batch size\n",
      "Target type on batch size = <class 'torch.Tensor'>\n",
      "Target shape on batch size =  torch.Size([1])\n",
      "1021\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING')\n",
    "\n",
    "img, lab_target = data_dataset['train'].__getitem__(0)\n",
    "\n",
    "print('DATASET')\n",
    "print('image at the first row: ', img.shape)\n",
    "print('image at the first row: ', type(img))\n",
    "print('image size at the first row: {}'.format(img.size()))\n",
    "print('\\nTarget at the first row: ', lab_target)\n",
    "print('Target format at the first row: {}'.format(lab_target))\n",
    "print('Target format at the first row: {}'.format(lab_target.shape))\n",
    "\n",
    "\n",
    "print()\n",
    "print('Train Loader type')\n",
    "train_iter = data_loader['train']\n",
    "print(type(train_iter))\n",
    "\n",
    "images = train_iter[0][0]\n",
    "labels_target = train_iter[0][1]\n",
    "print('DATALOADER')\n",
    "print('images type on batch size = {}'.format(type(images)))\n",
    "print('images shape on batch size = ', images.shape)\n",
    "print('\\nTargett type on batch size')\n",
    "print('Target type on batch size = {}'.format(type(labels_target)))\n",
    "print('Target shape on batch size = ', labels_target.shape)\n",
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "for idx, (data, target) in enumerate(data_loader['train']):\n",
    "    print(len(data))\n",
    "    next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        \n",
    "        # linear layer (330 -> 755001)\n",
    "        self.fc1 = nn.Linear(288, 1)\n",
    "        \n",
    "        # linear layer (500 -> 250)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        \n",
    "        '''# linear layer (250 -> 125)\n",
    "        self.fc3 = nn.Linear(125, 1)'''\n",
    "        \n",
    "        '''# linear layer (125 -> 1)\n",
    "        self.fc4 = nn.Linear(75, 1)'''\n",
    "        \n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.175)\n",
    "        \n",
    "        # LogSoftmax\n",
    "        #self.LSM = nn.LogSoftmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = self.fc1(x)\n",
    "        #x = F.tanh(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        #h2\n",
    "        #x = self.fc2(x)\n",
    "        #x = F.tanh(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        #h3\n",
    "        '''x = self.fc3(x)\n",
    "        #x = F.tanh(x)\n",
    "        #x = self.dropout(x)'''\n",
    "        \n",
    "        '''#h4\n",
    "        x = self.fc4(x)\n",
    "        #x = self.LSM(x)\n",
    "'''\n",
    "        return x\n",
    "\n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model_HR = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_patho.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=288, out_features=1, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.175, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: select loss function\n",
    "criterion = nn.MSELoss()\n",
    "'''def criterion(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()'''\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer = optim.SGD(model_HR.parameters(), lr=0.0001, momentum = 0.9)\n",
    "\n",
    "VERSION = 'Test_version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    time_start = time.time()\n",
    "    train_class = []\n",
    "    valid_class = []\n",
    "    epoch_class = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        LR = 0.01\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for idx, (data, target) in enumerate(loaders['train']):\n",
    "            \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += criterion(output, target).sum()\n",
    "            #print('train loss {2f}', train_loss)\n",
    "            \n",
    "            \n",
    "\n",
    "        model.eval()\n",
    "        for idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += criterion(output, target).sum()\n",
    "            \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(loaders['train'])\n",
    "        \n",
    "        valid_loss = valid_loss/len(loaders['valid'])\n",
    "        \n",
    "        '''if valid_loss < 0.35 and valid_loss > 0.15:\n",
    "            LR=0.005\n",
    "        elif valid_loss < 0.15:\n",
    "            LR=0.001'''\n",
    "        \n",
    "        # Calcul time\n",
    "        time_now = time.time()\n",
    "        \n",
    "        time_epoch = (time_now - time_start)/60\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTime since the beginning {:.1f} min \\tLearning rate: {:.6f} '.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            time_epoch,\n",
    "            LR\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss,\n",
    "            torch.save(model.state_dict(), VERSION))\n",
    "                 )\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "        # store class data\n",
    "        train_class.append(train_loss)\n",
    "        valid_class.append(valid_loss)\n",
    "        epoch_class.append(epoch)\n",
    "    \n",
    "    plt.plot(epoch_class, train_class, 'g', label='Training loss')\n",
    "    plt.plot(epoch_class, valid_class, 'b', label='validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 531276960.000000 \tValidation Loss: 1589484032.000000 \tTime since the beginning 0.0 min \tLearning rate: 0.010000 \n",
      "Validation loss decreased (inf --> 1589484032.000000).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 530317888.000000 \tValidation Loss: 1590632320.000000 \tTime since the beginning 0.0 min \tLearning rate: 0.010000 \n",
      "Epoch: 3 \tTraining Loss: 529381152.000000 \tValidation Loss: 1591776256.000000 \tTime since the beginning 0.0 min \tLearning rate: 0.010000 \n",
      "Epoch: 4 \tTraining Loss: 528465440.000000 \tValidation Loss: 1592914432.000000 \tTime since the beginning 0.1 min \tLearning rate: 0.010000 \n",
      "Epoch: 5 \tTraining Loss: 527569984.000000 \tValidation Loss: 1594049280.000000 \tTime since the beginning 0.1 min \tLearning rate: 0.010000 \n",
      "Epoch: 6 \tTraining Loss: 526694528.000000 \tValidation Loss: 1595177856.000000 \tTime since the beginning 0.1 min \tLearning rate: 0.010000 \n",
      "Epoch: 7 \tTraining Loss: 525838624.000000 \tValidation Loss: 1596300544.000000 \tTime since the beginning 0.1 min \tLearning rate: 0.010000 \n",
      "Epoch: 8 \tTraining Loss: 524999840.000000 \tValidation Loss: 1597420928.000000 \tTime since the beginning 0.1 min \tLearning rate: 0.010000 \n",
      "Epoch: 9 \tTraining Loss: 524179328.000000 \tValidation Loss: 1598532480.000000 \tTime since the beginning 0.1 min \tLearning rate: 0.010000 \n",
      "Epoch: 10 \tTraining Loss: 523375680.000000 \tValidation Loss: 1599640448.000000 \tTime since the beginning 0.2 min \tLearning rate: 0.010000 \n",
      "Epoch: 11 \tTraining Loss: 522588000.000000 \tValidation Loss: 1600741120.000000 \tTime since the beginning 0.2 min \tLearning rate: 0.010000 \n",
      "Epoch: 12 \tTraining Loss: 521816064.000000 \tValidation Loss: 1601836032.000000 \tTime since the beginning 0.2 min \tLearning rate: 0.010000 \n",
      "Epoch: 13 \tTraining Loss: 521059136.000000 \tValidation Loss: 1602926336.000000 \tTime since the beginning 0.2 min \tLearning rate: 0.010000 \n",
      "Epoch: 14 \tTraining Loss: 520317216.000000 \tValidation Loss: 1604011648.000000 \tTime since the beginning 0.2 min \tLearning rate: 0.010000 \n",
      "Epoch: 15 \tTraining Loss: 519589056.000000 \tValidation Loss: 1605087360.000000 \tTime since the beginning 0.2 min \tLearning rate: 0.010000 \n",
      "Epoch: 16 \tTraining Loss: 518874464.000000 \tValidation Loss: 1606158976.000000 \tTime since the beginning 0.3 min \tLearning rate: 0.010000 \n",
      "Epoch: 17 \tTraining Loss: 518172960.000000 \tValidation Loss: 1607223424.000000 \tTime since the beginning 0.3 min \tLearning rate: 0.010000 \n",
      "Epoch: 18 \tTraining Loss: 517484768.000000 \tValidation Loss: 1608282752.000000 \tTime since the beginning 0.3 min \tLearning rate: 0.010000 \n",
      "Epoch: 19 \tTraining Loss: 516808192.000000 \tValidation Loss: 1609334272.000000 \tTime since the beginning 0.3 min \tLearning rate: 0.010000 \n",
      "Epoch: 20 \tTraining Loss: 516144096.000000 \tValidation Loss: 1610379904.000000 \tTime since the beginning 0.3 min \tLearning rate: 0.010000 \n",
      "Epoch: 21 \tTraining Loss: 515490816.000000 \tValidation Loss: 1611419136.000000 \tTime since the beginning 0.3 min \tLearning rate: 0.010000 \n",
      "Epoch: 22 \tTraining Loss: 514849792.000000 \tValidation Loss: 1612452224.000000 \tTime since the beginning 0.4 min \tLearning rate: 0.010000 \n",
      "Epoch: 23 \tTraining Loss: 514218880.000000 \tValidation Loss: 1613478400.000000 \tTime since the beginning 0.4 min \tLearning rate: 0.010000 \n",
      "Epoch: 24 \tTraining Loss: 513598560.000000 \tValidation Loss: 1614497408.000000 \tTime since the beginning 0.4 min \tLearning rate: 0.010000 \n",
      "Epoch: 25 \tTraining Loss: 512988480.000000 \tValidation Loss: 1615511680.000000 \tTime since the beginning 0.4 min \tLearning rate: 0.010000 \n",
      "Epoch: 26 \tTraining Loss: 512388160.000000 \tValidation Loss: 1616516224.000000 \tTime since the beginning 0.4 min \tLearning rate: 0.010000 \n",
      "Epoch: 27 \tTraining Loss: 511797824.000000 \tValidation Loss: 1617516672.000000 \tTime since the beginning 0.4 min \tLearning rate: 0.010000 \n",
      "Epoch: 28 \tTraining Loss: 511216512.000000 \tValidation Loss: 1618509312.000000 \tTime since the beginning 0.5 min \tLearning rate: 0.010000 \n",
      "Epoch: 29 \tTraining Loss: 510644576.000000 \tValidation Loss: 1619495680.000000 \tTime since the beginning 0.5 min \tLearning rate: 0.010000 \n",
      "Epoch: 30 \tTraining Loss: 510081056.000000 \tValidation Loss: 1620475136.000000 \tTime since the beginning 0.5 min \tLearning rate: 0.010000 \n",
      "Epoch: 31 \tTraining Loss: 509526400.000000 \tValidation Loss: 1621447552.000000 \tTime since the beginning 0.5 min \tLearning rate: 0.010000 \n",
      "Epoch: 32 \tTraining Loss: 508980128.000000 \tValidation Loss: 1622413568.000000 \tTime since the beginning 0.5 min \tLearning rate: 0.010000 \n",
      "Epoch: 33 \tTraining Loss: 508441760.000000 \tValidation Loss: 1623372416.000000 \tTime since the beginning 0.5 min \tLearning rate: 0.010000 \n",
      "Epoch: 34 \tTraining Loss: 507911488.000000 \tValidation Loss: 1624326784.000000 \tTime since the beginning 0.6 min \tLearning rate: 0.010000 \n",
      "Epoch: 35 \tTraining Loss: 507389024.000000 \tValidation Loss: 1625272064.000000 \tTime since the beginning 0.6 min \tLearning rate: 0.010000 \n",
      "Epoch: 36 \tTraining Loss: 506873920.000000 \tValidation Loss: 1626211840.000000 \tTime since the beginning 0.6 min \tLearning rate: 0.010000 \n",
      "Epoch: 37 \tTraining Loss: 506365792.000000 \tValidation Loss: 1627144832.000000 \tTime since the beginning 0.6 min \tLearning rate: 0.010000 \n",
      "Epoch: 38 \tTraining Loss: 505865312.000000 \tValidation Loss: 1628071168.000000 \tTime since the beginning 0.6 min \tLearning rate: 0.010000 \n",
      "Epoch: 39 \tTraining Loss: 505371328.000000 \tValidation Loss: 1628990976.000000 \tTime since the beginning 0.6 min \tLearning rate: 0.010000 \n",
      "Epoch: 40 \tTraining Loss: 504884736.000000 \tValidation Loss: 1629904896.000000 \tTime since the beginning 0.6 min \tLearning rate: 0.010000 \n",
      "Epoch: 41 \tTraining Loss: 504404288.000000 \tValidation Loss: 1630810368.000000 \tTime since the beginning 0.7 min \tLearning rate: 0.010000 \n",
      "Epoch: 42 \tTraining Loss: 503929856.000000 \tValidation Loss: 1631710720.000000 \tTime since the beginning 0.7 min \tLearning rate: 0.010000 \n",
      "Epoch: 43 \tTraining Loss: 503462432.000000 \tValidation Loss: 1632604416.000000 \tTime since the beginning 0.7 min \tLearning rate: 0.010000 \n",
      "Epoch: 44 \tTraining Loss: 503001184.000000 \tValidation Loss: 1633491968.000000 \tTime since the beginning 0.7 min \tLearning rate: 0.010000 \n",
      "Epoch: 45 \tTraining Loss: 502545600.000000 \tValidation Loss: 1634370944.000000 \tTime since the beginning 0.7 min \tLearning rate: 0.010000 \n",
      "Epoch: 46 \tTraining Loss: 502096224.000000 \tValidation Loss: 1635246208.000000 \tTime since the beginning 0.7 min \tLearning rate: 0.010000 \n",
      "Epoch: 47 \tTraining Loss: 501651872.000000 \tValidation Loss: 1636115072.000000 \tTime since the beginning 0.8 min \tLearning rate: 0.010000 \n",
      "Epoch: 48 \tTraining Loss: 501213568.000000 \tValidation Loss: 1636977024.000000 \tTime since the beginning 0.8 min \tLearning rate: 0.010000 \n",
      "Epoch: 49 \tTraining Loss: 500780864.000000 \tValidation Loss: 1637832960.000000 \tTime since the beginning 0.8 min \tLearning rate: 0.010000 \n",
      "Epoch: 50 \tTraining Loss: 500353344.000000 \tValidation Loss: 1638681344.000000 \tTime since the beginning 0.8 min \tLearning rate: 0.010000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_res = train(50, data_loader, model_HR, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_iter\n",
      " <iterator object at 0x000001E039DB6188>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'iterator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-226-79e9e100fd90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataiter_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_iter\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataiter_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataiter_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata_test_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image test data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_test_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'iterator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "dataiter_test = iter(data_loader['test'])\n",
    "print('data_iter\\n',dataiter_test)\n",
    "data_test = dataiter_test.next()\n",
    "data_test_data = data_test[0]\n",
    "print('image test data',data_test_data)\n",
    "\n",
    "model_test = model_HR\n",
    "model_test.load_state_dict(torch.load(VERSION))\n",
    "model_test = model_test.eval()\n",
    "\n",
    "\n",
    "out_fwd = model_test.forward(data_test_data)\n",
    "print('Result preditcion model on dataset:\\n {}\\n'.format(out_fwd))\n",
    "probs = torch.exp(out_fwd)\n",
    "print('probs\\n', probs)\n",
    "print(probs.max())\n",
    "print(probs.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
